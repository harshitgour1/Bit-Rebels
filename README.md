# ðŸ¤– SahaayAI â€“ A Friend That Speaks, Sees, and Signs
## Github repo : (https://github.com/harshitgour1/Bit-Rebels.git)
# Drive link : https://drive.google.com/drive/folders/1K3GQ0AmmxLmvteqeVRBhrgTEN9dl1u3m?usp=sharing

SahaayAI (from Sanskrit "SahÄya" meaning 'companion' or 'helper') is an AI-powered accessibility assistant built to empower individuals who are Deaf, Hard of Hearing, Visually Impaired, or Speech-Impaired. It brings together cutting-edge language models, computer vision, and assistive technologies into one inclusive experience.

>  A Friend That Speaks, Sees, and Signs

---

## ðŸŒŸ Features

### ðŸ–ï¸ Sign2Speak
Real-time Sign Language Recognition  
âž¡ï¸ Uses computer vision and MediaPipe to recognize ASL signs  
âž¡ï¸ Converts them into spoken words and readable text  
âž¡ï¸ Empowers Deaf/Hard of Hearing individuals in voice-based environments

### ðŸ’¬ TalkBuddy
Text-to-Speech Assistant  
âž¡ï¸ Type what you want to say  
âž¡ï¸ Generates human-like speech using Google TTS or pyttsx3  
âž¡ï¸ Ideal for speech-impaired individuals

### ðŸ–¼ï¸ SeeForMe
AI-Powered Visual Understanding  
âž¡ï¸ Upload or capture an image  
âž¡ï¸ Describes the scene using OpenAI GPT-4o & BLIP  
âž¡ï¸ Text-to-Speech reads it aloud for the visually impaired

### ðŸ§­ AccessAware Navigation (Coming Soon)  
Context-aware DLIT assistant to help users navigate spaces based on their unique needs

---
!![image](https://github.com/user-attachments/assets/8241082d-59d0-4103-925e-150e7dfcd2fe)

Workflows

## Feature Workflow
### ðŸ–ï¸ *Sign2Speak* â€“ Real-Time ASL Recognition

1. *Input:* User performs ASL gestures via webcam.
2. *Recognition:* *MediaPipe* tracks hand gestures in real-time.
3. *Conversion:* Gestures are mapped to ASL signs and converted to text.
4. *Output:* Text is converted to speech using *Google TTS* or *pyttsx3*.

*Technologies:* MediaPipe, Google TTS/pyttsx3, FastAPI

![image](https://github.com/user-attachments/assets/239172ab-ef65-477d-8abb-7fe866ab5c08)

---

### ðŸ’¬ *TalkBuddy* â€“ Text-to-Speech Assistant

1. *Input:* User types text.
2. *Conversion:* Text is converted to speech by *Google TTS* or *pyttsx3*.
3. *Output:* Speech is played aloud for the user.

*Technologies:* Google TTS/pyttsx3, React.js, FastAPI

---

### ðŸ–¼ï¸ *SeeForMe* â€“ Visual Understanding for the Blind

1. *Input:* User uploads or captures an image.
2. *Processing:* *BLIP* generates a descriptive caption of the image.
3. *Output:* Caption is read aloud using *Google TTS* or *pyttsx3*.

*Technologies:* BLIP, Google TTS/pyttsx3, FastAPI

---

##  System Architecture

1. *Frontend (React.js):* Handles user input (e.g., text, images) and displays results.
2. *Backend (FastAPI):* Manages requests and processes data (e.g., sign recognition, image captioning).
3. *AI Models:* Use MediaPipe for ASL recognition and BLIP for image understanding.
4. *TTS Engine:* Converts text into speech.

---

## ðŸ’¡ Why sahaayAI?

Millions live with communication barriers every day. sahaayAI is designed to serve as:

- ðŸ—£ A voice for the speech-impaired  
- ðŸ‘€ Eyes for the blind  
- ðŸ‘‚ Understanding for the Deaf  

No more exclusion. Just accessible, adaptive, human-centered AI.

---
## ðŸ§­ SahaayAI â€“ User Flow Diagram

- ðŸ  Landing Page  
  - ðŸ”½ Feature Selection  
    - ðŸ”¹ Sign2Speak (ðŸ§)
      - ðŸ“· Activate webcam for live ASL input  
      - ðŸ“ Display recognized signs as text  
      - ðŸ”Š Convert recognized text into speech  
    - ðŸ”¹ TalkBuddy (ðŸ—£ï¸)
      - âŒ¨ï¸ User types a message  
      - ðŸ”Š Output spoken voice using TTS  
    - ðŸ”¹ SeeForMe (ðŸ–¼ï¸)
      - ðŸ“¤ Upload or capture an image  
      - ðŸ§  AI generates a description  
        - ðŸ“ Display text caption  
        - ðŸ”Š Read aloud via TTS  

  - âš™ï¸ Accessibility Settings  
    - Toggle options:
      - Font Size  
      - High Contrast Mode  
      - Keyboard-Only Navigation  
      - Voice Output Options  

  - â„¹ï¸ About / Team / Help

## ðŸ› ï¸ Tech Stack

| Layer        | Technologies                                                  |
|--------------|---------------------------------------------------------------|
| Frontend     | React.js, TailwindCSS, Axios                                  |
| Backend      | FastAPI (Python), Uvicorn, DLIT Model                         |
| AI Models    | OpenAI GPT-4o, BLIP (Image Captioning), MediaPipe for ASL     |
| TTS Engine   | Google Text-to-Speech, pyttsx3                                |
| Prototyping  | Streamlit (used for rapid UI/testing during development)      |

---

## âš™ï¸ Installation & Setup

### ðŸ“ Clone the Repo

```bash
git clone https://github.com/harshitgour1/Bit-Rebels.git
cd impact-hackathon 

Get the backend code from the drive and copy that folder in impact-hacthon

after that:
Run the Frontend by npm run dev

For the Backend:
cd backend

python -m venv venv       
>> .\venv\Scripts\Activate.ps1
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
pip install fastapi uvicorn[standard] transformers torch pillow pyttsx3 python-multipart
after this run this python main.py



```
## Demo On Drive

ðŸ“‚ **Full Demo Files & Screenshots:**  
[View on Google Drive](https://drive.google.com/drive/folders/1K3GQ0AmmxLmvteqeVRBhrgTEN9dl1u3m?usp=sharing)
